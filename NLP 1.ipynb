{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53b577a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in C:\\Users\\Admin\\Downloads\\Audio_file_2.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Python code to convert video to audio\n",
    "import moviepy.editor as mp\n",
    "# Insert Local Video File Path\n",
    "clip = mp.VideoFileClip(r\"C:\\Users\\Admin\\Downloads\\video.mp4\")\n",
    "\n",
    "# Insert Local Audio File Path\n",
    "audio_clip= clip.audio.write_audiofile(r\"C:\\Users\\Admin\\Downloads\\Audio_file_2.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0bceced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "import speech_recognition as sr\n",
    "r = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32948c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_large_audio_transcription(path):\n",
    "    \"\"\"\n",
    "    Splitting the large audio file into chunks\n",
    "    and apply speech recognition on each of these chunks\n",
    "    \"\"\"\n",
    "    # open the audio file using pydub\n",
    "    sound = AudioSegment.from_wav(path)  \n",
    "    # split audio sound where silence is 700 miliseconds or more and get chunks\n",
    "    chunks = split_on_silence(sound,\n",
    "        # experiment with this value for your target audio file\n",
    "        min_silence_len = 500,\n",
    "        # adjust this per requirement\n",
    "        silence_thresh = sound.dBFS-14,\n",
    "        # keep the silence for 1 second, adjustable as well\n",
    "        keep_silence=500,\n",
    "    )\n",
    "    folder_name = \"audio-chunks\"\n",
    "    # create a directory to store the audio chunks\n",
    "    if not os.path.isdir(folder_name):\n",
    "        os.mkdir(folder_name)\n",
    "    whole_text = \"\"\n",
    "    # process each chunk \n",
    "    for i, audio_chunk in enumerate(chunks, start=1):\n",
    "        # export audio chunk and save it in\n",
    "        # the `folder_name` directory.\n",
    "        chunk_filename = os.path.join(folder_name, f\"chunk{i}.wav\")\n",
    "        audio_chunk.export(chunk_filename, format=\"wav\")\n",
    "        # recognize the chunk\n",
    "        with sr.AudioFile(chunk_filename) as source:\n",
    "            audio_listened = r.record(source)\n",
    "            # try converting it to text\n",
    "            try:\n",
    "                text = r.recognize_google(audio_listened)\n",
    "            except sr.UnknownValueError as e:\n",
    "                print(\"Error:\", str(e))\n",
    "            else:\n",
    "                text = f\"{text.capitalize()}. \"\n",
    "                print(chunk_filename, \":\", text)\n",
    "                whole_text += text\n",
    "    # return the text for all chunks detected\n",
    "    return whole_text        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e629c058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: \n",
      "Error: \n",
      "audio-chunks\\chunk3.wav : Human beings are the most advanced species on earth there is no doubt in that. \n",
      "audio-chunks\\chunk4.wav : Success as human beings is because of our ability to communicate and share information. \n",
      "audio-chunks\\chunk5.wav : The concept of developing a language counsel. \n",
      "audio-chunks\\chunk6.wav : And when you talk about the human language it is one of the most diverse and complex part of a. \n",
      "audio-chunks\\chunk7.wav : Considering a total of 6500 languages that exist. \n",
      "audio-chunks\\chunk8.wav : Coming to the 21st century according to the industry estimate. \n",
      "audio-chunks\\chunk9.wav : Only 21% of the available data is present in the structured form. \n",
      "audio-chunks\\chunk10.wav : Geeta ismein chandra itada speak to it and send messages on whatsapp the radius of the groups of facebook. \n",
      "audio-chunks\\chunk11.wav : And maturity of the state are accessed in the textual form which is highly structured in nature. \n",
      "audio-chunks\\chunk12.wav : Now in order to produce significant and actionable insights from the state it is important to get acquainted with the techniques of text analysis and natural language processing. \n",
      "audio-chunks\\chunk13.wav : Understand what is text mining and natural language processing. \n",
      "audio-chunks\\chunk14.wav : Text mining on text analytics is the process of deriving meaningful information from natural language text it usually involves the process of structuring the input tax. \n",
      "audio-chunks\\chunk15.wav : Driving patterns with structured data and final interpreter. \n",
      "audio-chunks\\chunk16.wav : On the other hand natural language processing refers to the artificial intelligence method of communicating pattern intelligence system using the natural language. \n",
      "audio-chunks\\chunk17.wav : Text mining refers to the process of deriving high quality information from the text. \n",
      "audio-chunks\\chunk18.wav : Call is essentially turn the text into data analysis via the application of natural language processing. \n",
      "audio-chunks\\chunk19.wav : Text mining and nlp ko hand-in-hand understand some of the applications of text mining on natural language processing. \n",
      "audio-chunks\\chunk20.wav : One of the first and the most important applications of natural language processing is sentiment analysis. \n",
      "audio-chunks\\chunk21.wav : Twitter sentiment analysis the facebook sentiment is being used happily. \n",
      "audio-chunks\\chunk22.wav : Next we have the implementation of chatbot now you might have used the customer care services provided by various companies and the process behind all of that is because of the nlp. \n",
      "audio-chunks\\chunk23.wav : We are speech recognition and here we also talk about the voice assistants like siri google assistant and cortana and the process behind all of this is because of the natural language processing. \n",
      "audio-chunks\\chunk24.wav : Machine translation is also another use case of natural language processing and the most common example for it is the google translate which use nlp to translate data from one language to another and that too in the real time. \n",
      "audio-chunks\\chunk25.wav : Applications of penalty in truth spell checking keyword search and also extracting information from any dog or any website and finally one of the coolest application of natural language processing is advertise on matching. \n",
      "audio-chunks\\chunk26.wav : Basic recommendation of ads based on your history. \n",
      "audio-chunks\\chunk27.wav : Nlp is divided into two major components. \n",
      "audio-chunks\\chunk28.wav : Tourist in natural language understanding and natural language generation. \n",
      "audio-chunks\\chunk29.wav : Understanding generally refers to mapping the given input into natural language into useful representation and analysing those aspects of language. \n",
      "audio-chunks\\chunk30.wav : Restoration is the process of producing meaningful phrases and sentences in the form of natural language from some internal representation. \n",
      "audio-chunks\\chunk31.wav : Natural language understanding is usually hard and other natural language generation because it takes a lot of time and a lot of things to usually understand a particular language. \n",
      "audio-chunks\\chunk32.wav : Especially if you are not a human being. \n",
      "audio-chunks\\chunk33.wav : Now there are various steps involved in the natural language processing which are tokenization stemming lemmatization. \n",
      "audio-chunks\\chunk34.wav : Us tax name entity recognition and chunking. \n",
      "audio-chunks\\chunk35.wav : Starting with tokenization tokenization is a process of breaking strings into tokens which in turn are small structures or units that can be used for tokenization. \n",
      "audio-chunks\\chunk36.wav : We have a look at the example here taking this sentence into consideration it can be divided into 7 tokens. \n",
      "audio-chunks\\chunk37.wav : This is very useful in the natural language processing part. \n",
      "audio-chunks\\chunk38.wav : Capital s process in natural language processing is coming. \n",
      "audio-chunks\\chunk39.wav : Mastering usually refers to normalising the words into its base of the root form sippy have a look at the world we have effect asian effects affections affected affection and effect. \n",
      "audio-chunks\\chunk40.wav : All of these word originate from a single root word. \n",
      "audio-chunks\\chunk41.wav : As you might have guessed. \n",
      "audio-chunks\\chunk42.wav : It is effect. \n",
      "audio-chunks\\chunk43.wav : Stemming algorithm works by cutting of the end or the beginning of the word. \n",
      "audio-chunks\\chunk44.wav : Taking into account a list of common prefix suffixes that can be found in infected word. \n",
      "audio-chunks\\chunk45.wav : This indiscriminate cutting can be successful in some occasions but not always. \n",
      "audio-chunks\\chunk46.wav : So let's understand the concept of lemmatization now. \n",
      "audio-chunks\\chunk47.wav : Lemmatization on the other hand takes into consideration the morphological analysis of the world. \n",
      "audio-chunks\\chunk48.wav : Do so it is necessary to have a detailed dictionary which the algorithm can look through to link the form back to its original word are the root word which is also known as lemon. \n",
      "audio-chunks\\chunk49.wav : What lemmatization charges group together to find infected forms of the word call mr. \n",
      "audio-chunks\\chunk50.wav : At a somewhat similar to stemming acid mein saral words into 1 comment. \n",
      "audio-chunks\\chunk51.wav : The major difference between stemming and lemmatization is that the output of the lemmatization is a proper word for example a lemmatizer should map the word gone going and went in to go. \n",
      "audio-chunks\\chunk52.wav : That will not be the output for stemming. \n",
      "audio-chunks\\chunk53.wav : Once we have the tokens and once we have divided the tokens into its root form next comes the pos tags. \n",
      "audio-chunks\\chunk54.wav : Speak in the grammatical type of the word is referred to as pos tax or other parts of speech. \n",
      "audio-chunks\\chunk55.wav : Bs the verb noun adjective adverb article and many more it indicates how a word functions in meaning as well as grammatically within the sentence. \n",
      "audio-chunks\\chunk56.wav : About can have more than one part of speech based on the context in which it is used. \n",
      "audio-chunks\\chunk57.wav : For example let's take this sentence google something on the internet. \n",
      "audio-chunks\\chunk58.wav : Hey google is used as a verb although it's a proper noun. \n",
      "audio-chunks\\chunk59.wav : These are some of the limitations of icse the problems that occur while processing the natural language. \n",
      "audio-chunks\\chunk60.wav : Now to overcome all of these challenges. \n",
      "audio-chunks\\chunk61.wav : We have the name entity recognition. \n",
      "audio-chunks\\chunk62.wav : Also known as ntr. \n",
      "audio-chunks\\chunk63.wav : So it is the process of detecting the name and disease such as the person name the company names. \n",
      "audio-chunks\\chunk64.wav : We have the quantities of the location. \n",
      "audio-chunks\\chunk65.wav : It has three steps which are the noun phrase identification. \n",
      "audio-chunks\\chunk66.wav : The free state of occasion and entity disambiguation. \n",
      "audio-chunks\\chunk67.wav : Look at this particular example hey google ceo sundar pichai introduces the new pixel 3 at new york central mall. \n",
      "audio-chunks\\chunk68.wav : Chachi kaisi hai google is identified as a organisation sundar pichai as a person. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio-chunks\\chunk69.wav : We have new york as location and central mall is also defined as an organisation. \n",
      "audio-chunks\\chunk70.wav : Namaz we have divided the sentences into tok install the stemming and lemmatization added the tags and the name entity recognition it's time for us to get back together and make sense out of it. \n",
      "audio-chunks\\chunk71.wav : Chongqing. \n",
      "audio-chunks\\chunk72.wav : Chanting basically means breaking up individual pieces of information and dropping them together into the bigger pieces. \n",
      "audio-chunks\\chunk73.wav : Now these bk pieces are also known as chauk. \n",
      "audio-chunks\\chunk74.wav : In the context of nlp chongqing means grouping of words or tokens into chance. \n",
      "audio-chunks\\chunk75.wav : Pregnancy heavy have pink as an adjective panther as a noun and the as a reminder. \n",
      "audio-chunks\\chunk76.wav : And all of these are together trunk into a noun phrase. \n",
      "audio-chunks\\chunk77.wav : Nuts helps in getting insights. \n",
      "audio-chunks\\chunk78.wav : I mean full information. \n",
      "audio-chunks\\chunk79.wav : From the given text. \n",
      "audio-chunks\\chunk80.wav : You might be wondering where does one execute or run all of these programs. \n",
      "audio-chunks\\chunk81.wav : And all this function on a given text file. \n",
      "audio-chunks\\chunk82.wav : Python came up with nltk. \n",
      "audio-chunks\\chunk83.wav : Now what is nltk in nltk is the natural language toolkit library which is heavily used for all the natural language processing and the text analysis. \n",
      "audio-chunks\\chunk84.wav : So guys want to know the details about how to execute each and every parts like tokenization stemming lemmatization through nltk. \n",
      "audio-chunks\\chunk85.wav : You can refer to our nlp tutorial coming to which is given in the description box below children thank you and happy run. \n",
      "audio-chunks\\chunk86.wav : I hope you've enjoyed listening to this period please be kind enough to like it and you can comment any of your doubts and queries and we will reply them at the earliest. \n",
      "audio-chunks\\chunk87.wav : Look out for more videos in a playlist. \n",
      "audio-chunks\\chunk88.wav : And subscribe to edureka channel to learn more. \n",
      "audio-chunks\\chunk89.wav : Happy learning. \n"
     ]
    }
   ],
   "source": [
    "text = get_large_audio_transcription(r\"C:\\Users\\Admin\\Downloads\\Audio_file_2.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3380232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.3.0-cp39-cp39-win_amd64.whl (11.6 MB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.7-cp39-cp39-win_amd64.whl (18 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (21.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp39-cp39-win_amd64.whl (36 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.3-cp39-cp39-win_amd64.whl (448 kB)\n",
      "Collecting wasabi<1.1.0,>=0.9.1\n",
      "  Downloading wasabi-0.9.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (58.0.4)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (1.20.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.26.0)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.7-cp39-cp39-win_amd64.whl (6.6 MB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp39-cp39-win_amd64.whl (1.9 MB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.9\n",
      "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
      "Collecting thinc<8.1.0,>=8.0.14\n",
      "  Downloading thinc-8.0.15-cp39-cp39-win_amd64.whl (1.0 MB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.6-cp39-cp39-win_amd64.whl (112 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Collecting smart-open<6.0.0,>=5.0.0\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.10.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Installing collected packages: murmurhash, cymem, catalogue, wasabi, typer, srsly, smart-open, pydantic, preshed, blis, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
      "Successfully installed blis-0.7.7 catalogue-2.0.7 cymem-2.0.6 langcodes-3.3.0 murmurhash-1.0.7 pathy-0.6.1 preshed-3.0.6 pydantic-1.8.2 smart-open-5.2.1 spacy-3.3.0 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 thinc-8.0.15 typer-0.4.1 wasabi-0.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9b9a294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "#from spacy.en import English\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5a92a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_words=list(STOP_WORDS)+list(punctuation)+['\\n']\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "docx = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b823c335",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words=[word.text for word in docx]\n",
    "Freq_word={}\n",
    "for w in all_words:\n",
    "    w1=w.lower()\n",
    "    if w1 not in extra_words and w1.isalpha():\n",
    "        if w1 in Freq_word.keys():\n",
    "            Freq_word[w1]+=1\n",
    "        else:\n",
    "            Freq_word[w1]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e765a027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'human': 4,\n",
       " 'beings': 2,\n",
       " 'advanced': 1,\n",
       " 'species': 1,\n",
       " 'earth': 1,\n",
       " 'doubt': 1,\n",
       " 'success': 1,\n",
       " 'ability': 1,\n",
       " 'communicate': 1,\n",
       " 'share': 1,\n",
       " 'information': 6,\n",
       " 'concept': 2,\n",
       " 'developing': 1,\n",
       " 'language': 28,\n",
       " 'counsel': 1,\n",
       " 'talk': 2,\n",
       " 'diverse': 1,\n",
       " 'complex': 1,\n",
       " 'considering': 1,\n",
       " 'total': 1,\n",
       " 'languages': 1,\n",
       " 'exist': 1,\n",
       " 'coming': 3,\n",
       " 'century': 1,\n",
       " 'according': 1,\n",
       " 'industry': 1,\n",
       " 'estimate': 1,\n",
       " 'available': 1,\n",
       " 'data': 4,\n",
       " 'present': 1,\n",
       " 'structured': 3,\n",
       " 'form': 6,\n",
       " 'geeta': 1,\n",
       " 'ismein': 1,\n",
       " 'chandra': 1,\n",
       " 'itada': 1,\n",
       " 'speak': 2,\n",
       " 'send': 1,\n",
       " 'messages': 1,\n",
       " 'whatsapp': 1,\n",
       " 'radius': 1,\n",
       " 'groups': 1,\n",
       " 'facebook': 2,\n",
       " 'maturity': 1,\n",
       " 'state': 3,\n",
       " 'accessed': 1,\n",
       " 'textual': 1,\n",
       " 'highly': 1,\n",
       " 'nature': 1,\n",
       " 'order': 1,\n",
       " 'produce': 1,\n",
       " 'significant': 1,\n",
       " 'actionable': 1,\n",
       " 'insights': 2,\n",
       " 'important': 2,\n",
       " 'acquainted': 1,\n",
       " 'techniques': 1,\n",
       " 'text': 13,\n",
       " 'analysis': 6,\n",
       " 'natural': 23,\n",
       " 'processing': 14,\n",
       " 'understand': 4,\n",
       " 'mining': 5,\n",
       " 'analytics': 1,\n",
       " 'process': 9,\n",
       " 'deriving': 2,\n",
       " 'meaningful': 2,\n",
       " 'usually': 4,\n",
       " 'involves': 1,\n",
       " 'structuring': 1,\n",
       " 'input': 2,\n",
       " 'tax': 3,\n",
       " 'driving': 1,\n",
       " 'patterns': 1,\n",
       " 'final': 1,\n",
       " 'interpreter': 1,\n",
       " 'hand': 4,\n",
       " 'refers': 4,\n",
       " 'artificial': 1,\n",
       " 'intelligence': 2,\n",
       " 'method': 1,\n",
       " 'communicating': 1,\n",
       " 'pattern': 1,\n",
       " 'system': 1,\n",
       " 'high': 1,\n",
       " 'quality': 1,\n",
       " 'essentially': 1,\n",
       " 'turn': 2,\n",
       " 'application': 2,\n",
       " 'nlp': 6,\n",
       " 'ko': 1,\n",
       " 'applications': 3,\n",
       " 'sentiment': 3,\n",
       " 'twitter': 1,\n",
       " 'happily': 1,\n",
       " 'implementation': 1,\n",
       " 'chatbot': 1,\n",
       " 'customer': 1,\n",
       " 'care': 1,\n",
       " 'services': 1,\n",
       " 'provided': 1,\n",
       " 'companies': 1,\n",
       " 'speech': 3,\n",
       " 'recognition': 4,\n",
       " 'voice': 1,\n",
       " 'assistants': 1,\n",
       " 'like': 3,\n",
       " 'siri': 1,\n",
       " 'google': 6,\n",
       " 'assistant': 1,\n",
       " 'cortana': 1,\n",
       " 'machine': 1,\n",
       " 'translation': 1,\n",
       " 'use': 2,\n",
       " 'case': 1,\n",
       " 'common': 2,\n",
       " 'example': 5,\n",
       " 'translate': 2,\n",
       " 'real': 1,\n",
       " 'time': 3,\n",
       " 'penalty': 1,\n",
       " 'truth': 1,\n",
       " 'spell': 1,\n",
       " 'checking': 1,\n",
       " 'keyword': 1,\n",
       " 'search': 1,\n",
       " 'extracting': 1,\n",
       " 'dog': 1,\n",
       " 'website': 1,\n",
       " 'finally': 1,\n",
       " 'coolest': 1,\n",
       " 'advertise': 1,\n",
       " 'matching': 1,\n",
       " 'basic': 1,\n",
       " 'recommendation': 1,\n",
       " 'ads': 1,\n",
       " 'based': 2,\n",
       " 'history': 1,\n",
       " 'divided': 4,\n",
       " 'major': 2,\n",
       " 'components': 1,\n",
       " 'tourist': 1,\n",
       " 'understanding': 3,\n",
       " 'generation': 2,\n",
       " 'generally': 1,\n",
       " 'mapping': 1,\n",
       " 'given': 4,\n",
       " 'useful': 2,\n",
       " 'representation': 2,\n",
       " 'analysing': 1,\n",
       " 'aspects': 1,\n",
       " 'restoration': 1,\n",
       " 'producing': 1,\n",
       " 'phrases': 1,\n",
       " 'sentences': 2,\n",
       " 'internal': 1,\n",
       " 'hard': 1,\n",
       " 'takes': 2,\n",
       " 'lot': 2,\n",
       " 'things': 1,\n",
       " 'particular': 2,\n",
       " 'especially': 1,\n",
       " 'steps': 2,\n",
       " 'involved': 1,\n",
       " 'tokenization': 5,\n",
       " 'stemming': 7,\n",
       " 'lemmatization': 8,\n",
       " 'entity': 4,\n",
       " 'chunking': 1,\n",
       " 'starting': 1,\n",
       " 'breaking': 2,\n",
       " 'strings': 1,\n",
       " 'tokens': 5,\n",
       " 'small': 1,\n",
       " 'structures': 1,\n",
       " 'units': 1,\n",
       " 'look': 5,\n",
       " 'taking': 2,\n",
       " 'sentence': 3,\n",
       " 'consideration': 2,\n",
       " 'capital': 1,\n",
       " 's': 1,\n",
       " 'mastering': 1,\n",
       " 'normalising': 1,\n",
       " 'words': 3,\n",
       " 'base': 1,\n",
       " 'root': 4,\n",
       " 'sippy': 1,\n",
       " 'world': 2,\n",
       " 'effect': 3,\n",
       " 'asian': 1,\n",
       " 'effects': 1,\n",
       " 'affections': 1,\n",
       " 'affected': 1,\n",
       " 'affection': 1,\n",
       " 'word': 11,\n",
       " 'originate': 1,\n",
       " 'single': 1,\n",
       " 'guessed': 1,\n",
       " 'algorithm': 2,\n",
       " 'works': 1,\n",
       " 'cutting': 2,\n",
       " 'end': 1,\n",
       " 'beginning': 1,\n",
       " 'account': 1,\n",
       " 'list': 1,\n",
       " 'prefix': 1,\n",
       " 'suffixes': 1,\n",
       " 'found': 1,\n",
       " 'infected': 2,\n",
       " 'indiscriminate': 1,\n",
       " 'successful': 1,\n",
       " 'occasions': 1,\n",
       " 'let': 2,\n",
       " 'morphological': 1,\n",
       " 'necessary': 1,\n",
       " 'detailed': 1,\n",
       " 'dictionary': 1,\n",
       " 'link': 1,\n",
       " 'original': 1,\n",
       " 'known': 3,\n",
       " 'lemon': 1,\n",
       " 'charges': 1,\n",
       " 'group': 1,\n",
       " 'find': 1,\n",
       " 'forms': 1,\n",
       " 'mr': 1,\n",
       " 'somewhat': 1,\n",
       " 'similar': 1,\n",
       " 'acid': 1,\n",
       " 'mein': 1,\n",
       " 'saral': 1,\n",
       " 'comment': 2,\n",
       " 'difference': 1,\n",
       " 'output': 2,\n",
       " 'proper': 2,\n",
       " 'lemmatizer': 1,\n",
       " 'map': 1,\n",
       " 'gone': 1,\n",
       " 'going': 1,\n",
       " 'went': 1,\n",
       " 'comes': 1,\n",
       " 'pos': 2,\n",
       " 'tags': 2,\n",
       " 'grammatical': 1,\n",
       " 'type': 1,\n",
       " 'referred': 1,\n",
       " 'parts': 2,\n",
       " 'bs': 1,\n",
       " 'verb': 2,\n",
       " 'noun': 5,\n",
       " 'adjective': 2,\n",
       " 'adverb': 1,\n",
       " 'article': 1,\n",
       " 'indicates': 1,\n",
       " 'functions': 1,\n",
       " 'meaning': 1,\n",
       " 'grammatically': 1,\n",
       " 'context': 2,\n",
       " 'internet': 1,\n",
       " 'hey': 2,\n",
       " 'limitations': 1,\n",
       " 'icse': 1,\n",
       " 'problems': 1,\n",
       " 'occur': 1,\n",
       " 'overcome': 1,\n",
       " 'challenges': 1,\n",
       " 'ntr': 1,\n",
       " 'detecting': 1,\n",
       " 'disease': 1,\n",
       " 'person': 2,\n",
       " 'company': 1,\n",
       " 'names': 1,\n",
       " 'quantities': 1,\n",
       " 'location': 2,\n",
       " 'phrase': 2,\n",
       " 'identification': 1,\n",
       " 'free': 1,\n",
       " 'occasion': 1,\n",
       " 'disambiguation': 1,\n",
       " 'ceo': 1,\n",
       " 'sundar': 2,\n",
       " 'pichai': 2,\n",
       " 'introduces': 1,\n",
       " 'new': 3,\n",
       " 'pixel': 1,\n",
       " 'york': 2,\n",
       " 'central': 2,\n",
       " 'mall': 2,\n",
       " 'chachi': 1,\n",
       " 'kaisi': 1,\n",
       " 'hai': 1,\n",
       " 'identified': 1,\n",
       " 'organisation': 2,\n",
       " 'defined': 1,\n",
       " 'namaz': 1,\n",
       " 'tok': 1,\n",
       " 'install': 1,\n",
       " 'added': 1,\n",
       " 'sense': 1,\n",
       " 'chongqing': 2,\n",
       " 'chanting': 1,\n",
       " 'basically': 1,\n",
       " 'means': 2,\n",
       " 'individual': 1,\n",
       " 'pieces': 3,\n",
       " 'dropping': 1,\n",
       " 'bigger': 1,\n",
       " 'bk': 1,\n",
       " 'chauk': 1,\n",
       " 'grouping': 1,\n",
       " 'chance': 1,\n",
       " 'pregnancy': 1,\n",
       " 'heavy': 1,\n",
       " 'pink': 1,\n",
       " 'panther': 1,\n",
       " 'reminder': 1,\n",
       " 'trunk': 1,\n",
       " 'nuts': 1,\n",
       " 'helps': 1,\n",
       " 'getting': 1,\n",
       " 'mean': 1,\n",
       " 'wondering': 1,\n",
       " 'execute': 2,\n",
       " 'run': 2,\n",
       " 'programs': 1,\n",
       " 'function': 1,\n",
       " 'file': 1,\n",
       " 'python': 1,\n",
       " 'came': 1,\n",
       " 'nltk': 4,\n",
       " 'toolkit': 1,\n",
       " 'library': 1,\n",
       " 'heavily': 1,\n",
       " 'guys': 1,\n",
       " 'want': 1,\n",
       " 'know': 1,\n",
       " 'details': 1,\n",
       " 'refer': 1,\n",
       " 'tutorial': 1,\n",
       " 'description': 1,\n",
       " 'box': 1,\n",
       " 'children': 1,\n",
       " 'thank': 1,\n",
       " 'happy': 2,\n",
       " 'hope': 1,\n",
       " 'enjoyed': 1,\n",
       " 'listening': 1,\n",
       " 'period': 1,\n",
       " 'kind': 1,\n",
       " 'doubts': 1,\n",
       " 'queries': 1,\n",
       " 'reply': 1,\n",
       " 'earliest': 1,\n",
       " 'videos': 1,\n",
       " 'playlist': 1,\n",
       " 'subscribe': 1,\n",
       " 'edureka': 1,\n",
       " 'channel': 1,\n",
       " 'learn': 1,\n",
       " 'learning': 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Freq_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5d29fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic of document given :-\n",
      "language text natural processing "
     ]
    }
   ],
   "source": [
    "val=sorted(Freq_word.values())\n",
    "max_freq=val[-4:]\n",
    "print(\"Topic of document given :-\")\n",
    "for word,freq in Freq_word.items():\n",
    "    if freq in max_freq:\n",
    "        print(word ,end=\" \")\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f67b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in Freq_word.keys():\n",
    "       Freq_word[word] = (Freq_word[word]/max_freq[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bae46493",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_strength={}\n",
    "for sent in docx.sents:\n",
    "    for word in sent :\n",
    "        if word.text.lower() in Freq_word.keys():\n",
    "            if sent in sent_strength.keys():\n",
    "                sent_strength[sent]+=Freq_word[word.text.lower()]\n",
    "            else:\n",
    "                sent_strength[sent]=Freq_word[word.text.lower()]\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b144fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Human beings are the most advanced species on earth there is no doubt in that.: 0.3571428571428571,\n",
       " Success as human beings is because of our ability to communicate and share information.: 0.5714285714285714,\n",
       " The concept of developing a language counsel.: 1.142857142857143,\n",
       " And when you talk about the human language it is one of the most diverse and complex part of a.: 1.2857142857142858,\n",
       " Considering a total of 6500 languages that exist.: 0.14285714285714285,\n",
       " Coming to the 21st century according to the industry estimate.: 0.24999999999999994,\n",
       " Only 21% of the available data is present in the structured form.: 0.5357142857142857,\n",
       " Geeta ismein chandra itada speak to it and send messages on whatsapp the radius of the groups of facebook.: 0.4642857142857142,\n",
       " And maturity of the state are accessed in the textual form which is highly structured in nature.: 0.607142857142857,\n",
       " Now in order to produce significant and actionable insights from the state it is important to get acquainted with the techniques of text analysis and natural language processing.: 3.4642857142857144,\n",
       " Understand what is text mining and natural language processing.: 3.107142857142857,\n",
       " Text mining on text analytics is the process of deriving meaningful information from natural language text it usually involves the process of structuring the input tax.: 4.82142857142857,\n",
       " Driving patterns with structured data and final interpreter.: 0.3928571428571428,\n",
       " On the other hand natural language processing refers to the artificial intelligence method of communicating pattern intelligence system using the natural language.: 4.75,\n",
       " Text mining refers to the process of deriving high quality information from the text.: 1.9285714285714288,\n",
       " Call is essentially turn the text into data analysis via the application of natural language processing.: 3.321428571428571,\n",
       " Text mining and nlp ko hand-in-hand understand some of the applications of text mining on natural language processing.: 4.392857142857142,\n",
       " One of the first and the most important applications of natural language processing is sentiment analysis.: 2.8214285714285716,\n",
       " Twitter sentiment analysis the facebook sentiment is being used happily.: 0.5714285714285713,\n",
       " Next we have the implementation of chatbot now you might have used the customer care services provided by various companies and the process behind all of that is because of the nlp.: 0.7857142857142857,\n",
       " We are speech recognition and here we also talk about the voice assistants like siri google assistant and cortana and the process behind all of this is because of the natural language processing.: 3.4642857142857144,\n",
       " Machine translation is also another use case of natural language processing and the most common example for it is the google translate which use nlp to translate data from one language to another and that too in the real time.: 4.678571428571429,\n",
       " Applications of penalty in truth spell checking keyword search and also extracting information from any dog or any website and finally one of the coolest application of natural language processing is advertise on matching.: 3.178571428571428,\n",
       " Basic recommendation of ads based on your history.: 0.21428571428571425,\n",
       " Nlp is divided into two major components.: 0.4642857142857142,\n",
       " Tourist in natural language understanding and natural language generation.: 3.857142857142857,\n",
       " Understanding generally refers to mapping the given input into natural language into useful representation and analysing those aspects of language.: 3.5714285714285716,\n",
       " Restoration is the process of producing meaningful phrases and sentences in the form of natural language from some internal representation.: 2.7142857142857144,\n",
       " Natural language understanding is usually hard and other natural language generation because it takes a lot of time and a lot of things to usually understand a particular language.: 5.7142857142857135,\n",
       " Especially if you are not a human being.: 0.17857142857142855,\n",
       " Now there are various steps involved in the natural language processing which are tokenization stemming lemmatization.: 3.1428571428571423,\n",
       " Us tax name entity recognition and chunking.: 0.42857142857142855,\n",
       " Starting with tokenization tokenization is a process of breaking strings into tokens which in turn are small structures or units that can be used for tokenization.: 1.3571428571428574,\n",
       " We have a look at the example here taking this sentence into consideration it can be divided into 7 tokens.: 0.9285714285714286,\n",
       " This is very useful in the natural language processing part.: 2.392857142857143,\n",
       " Capital s process in natural language processing is coming.: 2.8214285714285716,\n",
       " Mastering usually refers to normalising the words into its base of the root form sippy have a look at the world we have effect asian effects affections affected affection and effect.: 1.5357142857142863,\n",
       " All of these word originate from a single root word.: 1.0,\n",
       " As you might have guessed.: 0.03571428571428571,\n",
       " It is effect.: 0.10714285714285714,\n",
       " Stemming algorithm works by cutting of the end or the beginning of the word.: 0.8928571428571428,\n",
       " Taking into account a list of common prefix suffixes that can be found in infected word.: 0.7857142857142856,\n",
       " This indiscriminate cutting can be successful in some occasions but not always.: 0.17857142857142855,\n",
       " So let's understand the concept of lemmatization now.: 0.5714285714285714,\n",
       " Lemmatization on the other hand takes into consideration the morphological analysis of the world.: 0.8928571428571428,\n",
       " Do so it is necessary to have a detailed dictionary which the algorithm can look through to link the form back to its original word are the root word which is also known as lemon.: 1.7142857142857142,\n",
       " What lemmatization charges group together to find infected forms of the word call mr.: 0.9285714285714285,\n",
       " At a somewhat similar to stemming acid mein saral words into 1 comment.: 0.607142857142857,\n",
       " The major difference between stemming and lemmatization is that the output of the lemmatization is a proper word for example a lemmatizer should map the word gone going and went in to go.: 2.214285714285714,\n",
       " That will not be the output for stemming.: 0.3214285714285714,\n",
       " Once we have the tokens and once we have divided the tokens into its root form next comes the pos tags.: 1.0357142857142856,\n",
       " Speak in the grammatical type of the word is referred to as pos tax or other parts of speech.: 0.9285714285714284,\n",
       " Bs the verb noun adjective adverb article and many more it indicates how a word functions in meaning as well as grammatically within the sentence.: 1.0714285714285712,\n",
       " About can have more than one part of speech based on the context in which it is used.: 0.24999999999999997,\n",
       " For example let's take this sentence google something on the internet.: 0.6071428571428571,\n",
       " Hey google is used as a verb although it's a proper noun.: 0.6071428571428571,\n",
       " These are some of the limitations of icse the problems that occur while processing the natural language.: 2.4642857142857144,\n",
       " Now to overcome all of these challenges.: 0.07142857142857142,\n",
       " We have the name entity recognition.: 0.2857142857142857,\n",
       " Also known as ntr.: 0.14285714285714285,\n",
       " So it is the process of detecting the name and disease such as the person name the company names.: 0.5357142857142857,\n",
       " We have the quantities of the location.: 0.10714285714285714,\n",
       " It has three steps which are the noun phrase identification.: 0.3571428571428571,\n",
       " The free state of occasion and entity disambiguation.: 0.3571428571428571,\n",
       " Look at this particular example hey google ceo sundar pichai introduces the new pixel 3 at new york central mall.: 1.3928571428571428,\n",
       " Chachi kaisi hai google is identified as a organisation sundar pichai as a person.: 0.6428571428571427,\n",
       " We have new york as location and central mall is also defined as an organisation.: 0.4999999999999999,\n",
       " Namaz we have divided the sentences into tok install the stemming and lemmatization added the tags and the name entity recognition: 1.2499999999999998,\n",
       " it's time for us to get back together and make sense out of it.: 0.14285714285714285,\n",
       " Chongqing.: 0.07142857142857142,\n",
       " Chanting basically means breaking up individual pieces of information and dropping them together into the bigger pieces.: 0.7499999999999999,\n",
       " Now these bk pieces are also known as chauk.: 0.2857142857142857,\n",
       " In the context of nlp chongqing means grouping of words or tokens into chance.: 0.7857142857142856,\n",
       " Pregnancy heavy have pink as an adjective panther as a noun and the as a reminder.: 0.4285714285714285,\n",
       " And all of these are together trunk into a noun phrase.: 0.2857142857142857,\n",
       " Nuts helps in getting insights.: 0.17857142857142855,\n",
       " I mean full information.: 0.25,\n",
       " From the given text.: 0.6071428571428572,\n",
       " You might be wondering where does one execute or run all of these programs.: 0.21428571428571425,\n",
       " And all this function on a given text file.: 0.6785714285714285,\n",
       " Python came up with nltk.: 0.21428571428571427,\n",
       " Now what is nltk in nltk is the natural language toolkit library which is heavily used for all the natural language processing and the text analysis.: 5.214285714285714,\n",
       " So guys want to know the details about how to execute each and every parts like tokenization stemming lemmatization through nltk.: 1.25,\n",
       " You can refer to our nlp tutorial coming to which is given in the description box below children thank you and happy run.: 0.8214285714285713,\n",
       " I hope you've enjoyed listening to this period please be kind enough to like it and you can comment any of your doubts and queries and we will reply them at the earliest.: 0.4999999999999999,\n",
       " Look out for more videos in a playlist.: 0.25,\n",
       " And subscribe to edureka channel to learn more.: 0.14285714285714285,\n",
       " Happy learning.: 0.10714285714285714}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e23c2b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_sentences=(sorted(sent_strength.values())[::-1])\n",
    "top20percent_sentence=int(0.2*len(top_sentences))\n",
    "top_sent=top_sentences[:top20percent_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68a222bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary=[]\n",
    "for sent,strength in sent_strength.items():\n",
    "    if strength in top_sent:\n",
    "        summary.append(sent)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a016081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now in order to produce significant and actionable insights from the state it is important to get acquainted with the techniques of text analysis and natural language processing.Understand what is text mining and natural language processing.Text mining on text analytics is the process of deriving meaningful information from natural language text it usually involves the process of structuring the input tax.On the other hand natural language processing refers to the artificial intelligence method of communicating pattern intelligence system using the natural language.Call is essentially turn the text into data analysis via the application of natural language processing.Text mining and nlp ko hand-in-hand understand some of the applications of text mining on natural language processing.One of the first and the most important applications of natural language processing is sentiment analysis.We are speech recognition and here we also talk about the voice assistants like siri google assistant and cortana and the process behind all of this is because of the natural language processing.Machine translation is also another use case of natural language processing and the most common example for it is the google translate which use nlp to translate data from one language to another and that too in the real time.Applications of penalty in truth spell checking keyword search and also extracting information from any dog or any website and finally one of the coolest application of natural language processing is advertise on matching.Tourist in natural language understanding and natural language generation.Understanding generally refers to mapping the given input into natural language into useful representation and analysing those aspects of language.Restoration is the process of producing meaningful phrases and sentences in the form of natural language from some internal representation.Natural language understanding is usually hard and other natural language generation because it takes a lot of time and a lot of things to usually understand a particular language.Now there are various steps involved in the natural language processing which are tokenization stemming lemmatization.Capital s process in natural language processing is coming.Now what is nltk in nltk is the natural language toolkit library which is heavily used for all the natural language processing and the text analysis."
     ]
    }
   ],
   "source": [
    "for i in summary:\n",
    "    print(i,end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a44e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
